{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.5"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Function to get S3 temporay credentials","metadata":{},"id":"75fea605-e63e-434e-9634-04531d831d78"},{"cell_type":"code","source":"def init_S3FileSystem():\n    \"\"\"\n    This routine automatically pull your EDL crediential from .netrc file and use it to obtain an AWS S3 credential through a podaac service accessable at https://archive.podaac.earthdata.nasa.gov/s3credentials\n    \n    Return:\n    =======\n    \n    s3: an AWS S3 filesystem\n    \"\"\"\n    import requests,s3fs\n    creds = requests.get('https://archive.podaac.earthdata.nasa.gov/s3credentials').json()\n    s3 = s3fs.S3FileSystem(anon=False,\n                           key=creds['accessKeyId'],\n                           secret=creds['secretAccessKey'], \n                           token=creds['sessionToken'])\n    return s3","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"5caa9ec6-e555-4916-84f0-d6185d99e9cb"},{"cell_type":"markdown","source":"# Class to subset data by space and time.","metadata":{},"id":"6248d054-6101-41e8-91a0-30523e6fa56f"},{"cell_type":"code","source":"# Standard imports\nimport netrc\n\nfrom urllib import request\nfrom http.cookiejar import CookieJar\nfrom socket import gethostname, gethostbyname\n\n# Third-party imports\nimport requests\n\nclass S3List:\n    \"\"\"Class used to query and download from PO.DAAC's CMR API.\n    \"\"\"\n\n    CMR = \"cmr.earthdata.nasa.gov\"\n    URS = \"urs.earthdata.nasa.gov\"\n\n    def __init__(self):\n        self._token = None\n\n    def login(self):\n        \"\"\"Log into Earthdata and set up request library to track cookies.\n        \n        Raises an exception if can't authenticate with .netrc file.\n        \"\"\"\n\n        try:\n            username, _, password = netrc.netrc().authenticators(self.URS)\n        except (FileNotFoundError, TypeError):\n            raise Exception(\"ERROR: There not .netrc file or endpoint indicated in .netrc file.\")\n\n        # Create Earthdata authentication request\n        manager = request.HTTPPasswordMgrWithDefaultRealm()\n        manager.add_password(None, self.URS, username, password)\n        auth = request.HTTPBasicAuthHandler(manager)\n\n        # Set up the storage of cookies\n        jar = CookieJar()\n        processor = request.HTTPCookieProcessor(jar)\n\n        # Define an opener to handle fetching auth request\n        opener = request.build_opener(auth, processor)\n        request.install_opener(opener)\n\n    def get_token(self, client_id, ip_address):\n        \"\"\"Get CMR authentication token for searching records.\n        \n        Parameters\n        ----------\n        client_id: str\n            client identifier to obtain token\n        ip_address: str\n            client's IP address\n        \"\"\"\n\n        try:\n            username, _, password = netrc.netrc().authenticators(self.URS)\n        except (FileNotFoundError, TypeError) as error:\n            raise Exception(\"ERROR: There not .netrc file or endpoint indicated in .netrc file.\")\n\n        # Post a token request and return resonse\n        token_url = f\"https://{self.CMR}/legacy-services/rest/tokens\"\n        token_xml = (f\"<token>\"\n                        f\"<username>{username}</username>\"\n                        f\"<password>{password}</password>\"\n                        f\"<client_id>{client_id}</client_id>\"\n                        f\"<user_ip_address>{ip_address}</user_ip_address>\"\n                    f\"</token>\")\n        headers = {\"Content-Type\" : \"application/xml\", \"Accept\" : \"application/json\"}\n        self._token = requests.post(url=token_url, data=token_xml, headers=headers) \\\n            .json()[\"token\"][\"id\"]\n\n    def delete_token(self):\n        \"\"\"Delete CMR authentication token.\"\"\"\n\n        token_url = f\"https://{self.CMR}/legacy-services/rest/tokens\"\n        headers = {\"Content-Type\" : \"application/xml\", \"Accept\" : \"application/json\"}\n        try:\n            res = requests.request(\"DELETE\", f\"{token_url}/{self._token}\", headers=headers)\n            return res.status_code\n        except Exception as e:\n            raise Exception(f\"Failed to delete token: {e}.\")\n\n    def run_query(self, shortname, provider, temporal_range, bbox):\n        \"\"\"Run query on collection referenced by shortname from provider.\"\"\"\n\n        url = f\"https://{self.CMR}/search/granules.umm_json\"\n        params = {\n                    \"provider\" : provider, \n                    \"ShortName\" : shortname, \n                    \"token\" : self._token,\n                    \"scroll\" : \"true\",\n                    \"page_size\" : 2000,\n                    \"sort_key\" : \"start_date\",\n                    \"temporal\" : temporal_range,\n                    \"bounding_box\": bbox,\n                    \"page_size\": 2000,\n                }\n        res = requests.get(url=url, params=params)        \n        coll = res.json()\n        return [url[\"URL\"] for res in coll[\"items\"] for url in res[\"umm\"][\"RelatedUrls\"] if url[\"Type\"] == \"GET DATA VIA DIRECT ACCESS\"]\n\n    def login_and_run_query(self, short_name, provider, temporal_range, bbox):\n        \"\"\"Log into CMR and run query to retrieve a list of S3 URLs.\"\"\"\n\n        try:\n            # Login and retrieve token\n            self.login()\n            client_id = \"podaac_cmr_client\"\n            hostname = gethostname()\n            ip_addr = gethostbyname(hostname)\n            self.get_token(client_id, ip_addr)\n\n            # Run query\n            s3_urls = self.run_query(short_name, provider, temporal_range, bbox)\n            s3_urls.sort()\n\n            # Clean up and delete token\n            self.delete_token()            \n        except Exception:\n            raise\n        else:\n            # Return list\n            return s3_urls","metadata":{"trusted":true},"execution_count":1,"outputs":[],"id":"46338b06-1738-45f8-adc7-2fee62c21e2c"},{"cell_type":"markdown","source":"# Define the S3 bucket for each satellite (useful?)","metadata":{},"id":"c3586a9f-f7c5-4c00-a22a-6702c5b9e06a"},{"cell_type":"code","source":"S3Buckets = {\n    'L2P S-NPP VIIRS'     : 's3://podaac-ops-cumulus-protected/VIIRS_NPP-STAR-L2P-v2.80', \n    'L2P MetopB AVHRR'    : 's3://podaac-ops-cumulus-protected/AVHRRF_MB-STAR-L2P-v2.80',\n    'L2P GCOM AMSR2'      : 's3://podaac-ops-cumulus-protected/AMSR2-REMSS-L2P-v8a',\n    'L2P GOES-16 '        : 's3://podaac-ops-cumulus-protected/ABI_G16-STAR-L2P-v2.70',\n    'L2P Meteosat SEVIRI' : 's3://podaac-ops-cumulus-protected/MSG03-OSPO-L2P-v1.0'\n}","metadata":{"trusted":true},"execution_count":20,"outputs":[],"id":"5de35c01-3489-4885-9b51-3908474d4482"},{"cell_type":"markdown","source":"# Use above class to retrieve S3 urls\n### But the last two satellites from `S3Buckets` return no files using the code below","metadata":{},"id":"e67f1e84-1293-40fd-8bf2-467bc547e8bd"},{"cell_type":"code","source":"import os\n\n# Required data \n# short_name = 'VIIRS_NPP-STAR-L2P-v2.80'\nprovider = 'POCLOUD'\ntemporal_range = '2022-07-18T00:00:00Z,2022-07-18T23:59:59Z'\nbbox = \"21,-64,66,-7\"\n\ns3_obj = S3List()\n\nfor sat in S3Buckets.values():\n    short_name = os.path.basename(sat)\n    \n    s3_urls = s3_obj.login_and_run_query(\n        short_name,\n        provider,\n        temporal_range,\n        bbox\n    )\n    print(len(s3_urls))","metadata":{"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"33\n35\n16\n0\n0\n","output_type":"stream"}],"id":"05d99175-7c55-42d6-aa79-1347e3f41cd7"},{"cell_type":"markdown","source":"# Open together all files for each satellite\n### By the [metop preprocessing notebook](https://github.com/gridSST-hackathon/data-pre-processing/blob/main/metop_preprocessing.ipynb), I'm in doubt if open all files together is the right thing to do","metadata":{},"id":"d68765a4-b58d-418f-9c06-7fad878e596b"},{"cell_type":"code","source":"import xarray as xr\n\n# from glob import glob\nfrom tqdm import tqdm\n\ns3 = init_S3FileSystem()\n\n# Iterate through remote_files to create a fileset\nfileset = [s3.open(file) for file in tqdm(s3_urls)]\n\n# # This works\n# data = xr.open_mfdataset(\n#     fileset,\n#     engine='h5netcdf'\n# )","metadata":{"trusted":true},"execution_count":38,"outputs":[],"id":"9b40f4eb-9840-4bee-bbd5-8cf254435cd1"},{"cell_type":"code","source":"data = xr.open_dataset(\n    fileset[0],\n    engine='h5netcdf'\n)\ndata","metadata":{"trusted":true},"execution_count":7,"outputs":[],"id":"2a7eef3a-9581-44ce-b6c2-2ce529bcbedb"}]}