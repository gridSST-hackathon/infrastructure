{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24bcd205-a373-4b97-b9d1-e4dc3f98dca1",
   "metadata": {},
   "source": [
    "# Pre-process module\n",
    "\n",
    "Includes:\n",
    "- A class that returns a list of S3 URLS for a dataset based on shortname, temporal range, and bounding box\n",
    "- Function to return temporary credentials for an S3 Bucket.\n",
    "- Function to clean and concatenate the data.\n",
    "- Function to write the data.\n",
    "- Function to run all operations and serve as container entrypoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ef0133-ae23-4bd7-94eb-e316a5f094df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "from http.cookiejar import CookieJar\n",
    "import netrc\n",
    "from socket import gethostname, gethostbyname\n",
    "from urllib import request\n",
    "\n",
    "# Third-party imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pylab as plt\n",
    "import requests\n",
    "import s3fs\n",
    "from tqdm import tqdm\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4cff35-f900-4c27-98ff-b54525ff66b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class S3List:\n",
    "    \"\"\"Class used to query and download from PO.DAAC's CMR API.\n",
    "    \"\"\"\n",
    "\n",
    "    CMR = \"cmr.earthdata.nasa.gov\"\n",
    "    URS = \"urs.earthdata.nasa.gov\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._token = None\n",
    "\n",
    "    def login(self):\n",
    "        \"\"\"Log into Earthdata and set up request library to track cookies.\n",
    "        \n",
    "        Raises an exception if can't authenticate with .netrc file.\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            username, _, password = netrc.netrc().authenticators(self.URS)\n",
    "        except (FileNotFoundError, TypeError):\n",
    "            raise Exception(\"ERROR: There not .netrc file or endpoint indicated in .netrc file.\")\n",
    "\n",
    "        # Create Earthdata authentication request\n",
    "        manager = request.HTTPPasswordMgrWithDefaultRealm()\n",
    "        manager.add_password(None, self.URS, username, password)\n",
    "        auth = request.HTTPBasicAuthHandler(manager)\n",
    "\n",
    "        # Set up the storage of cookies\n",
    "        jar = CookieJar()\n",
    "        processor = request.HTTPCookieProcessor(jar)\n",
    "\n",
    "        # Define an opener to handle fetching auth request\n",
    "        opener = request.build_opener(auth, processor)\n",
    "        request.install_opener(opener)\n",
    "\n",
    "    def get_token(self, client_id, ip_address):\n",
    "        \"\"\"Get CMR authentication token for searching records.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        client_id: str\n",
    "            client identifier to obtain token\n",
    "        ip_address: str\n",
    "            client's IP address\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            username, _, password = netrc.netrc().authenticators(self.URS)\n",
    "        except (FileNotFoundError, TypeError) as error:\n",
    "            raise Exception(\"ERROR: There not .netrc file or endpoint indicated in .netrc file.\")\n",
    "\n",
    "        # Post a token request and return resonse\n",
    "        token_url = f\"https://{self.CMR}/legacy-services/rest/tokens\"\n",
    "        token_xml = (f\"<token>\"\n",
    "                        f\"<username>{username}</username>\"\n",
    "                        f\"<password>{password}</password>\"\n",
    "                        f\"<client_id>{client_id}</client_id>\"\n",
    "                        f\"<user_ip_address>{ip_address}</user_ip_address>\"\n",
    "                    f\"</token>\")\n",
    "        headers = {\"Content-Type\" : \"application/xml\", \"Accept\" : \"application/json\"}\n",
    "        self._token = requests.post(url=token_url, data=token_xml, headers=headers) \\\n",
    "            .json()[\"token\"][\"id\"]\n",
    "\n",
    "    def delete_token(self):\n",
    "        \"\"\"Delete CMR authentication token.\"\"\"\n",
    "\n",
    "        token_url = f\"https://{self.CMR}/legacy-services/rest/tokens\"\n",
    "        headers = {\"Content-Type\" : \"application/xml\", \"Accept\" : \"application/json\"}\n",
    "        try:\n",
    "            res = requests.request(\"DELETE\", f\"{token_url}/{self._token}\", headers=headers)\n",
    "            return res.status_code\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Failed to delete token: {e}.\")\n",
    "\n",
    "    def run_query(self, shortname, provider, temporal_range, bbox):\n",
    "        \"\"\"Run query on collection referenced by shortname from provider.\"\"\"\n",
    "\n",
    "        url = f\"https://{self.CMR}/search/granules.umm_json\"\n",
    "        params = {\n",
    "                    \"provider\" : provider, \n",
    "                    \"ShortName\" : shortname, \n",
    "                    \"token\" : self._token,\n",
    "                    \"scroll\" : \"true\",\n",
    "                    \"page_size\" : 2000,\n",
    "                    \"sort_key\" : \"start_date\",\n",
    "                    \"temporal\" : temporal_range,\n",
    "                    \"bounding_box\": bbox,\n",
    "                    \"page_size\": 2000,\n",
    "                }\n",
    "        res = requests.get(url=url, params=params)        \n",
    "        coll = res.json()\n",
    "        return [url[\"URL\"] for res in coll[\"items\"] for url in res[\"umm\"][\"RelatedUrls\"] if url[\"Type\"] == \"GET DATA VIA DIRECT ACCESS\"]\n",
    "\n",
    "    def login_and_run_query(self, short_name, provider, temporal_range, bbox):\n",
    "        \"\"\"Log into CMR and run query to retrieve a list of S3 URLs.\"\"\"\n",
    "\n",
    "        try:\n",
    "            # Login and retrieve token\n",
    "            self.login()\n",
    "            client_id = \"podaac_cmr_client\"\n",
    "            hostname = gethostname()\n",
    "            ip_addr = gethostbyname(hostname)\n",
    "            self.get_token(client_id, ip_addr)\n",
    "\n",
    "            # Run query\n",
    "            s3_urls = self.run_query(short_name, provider, temporal_range, bbox)\n",
    "            s3_urls.sort()\n",
    "\n",
    "            # Clean up and delete token\n",
    "            self.delete_token()            \n",
    "        except Exception:\n",
    "            raise\n",
    "        else:\n",
    "            # Return list\n",
    "            return s3_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6905fe75-4a56-4a86-88be-6225bc0c1b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_S3FileSystem(provider):\n",
    "    \"\"\"\n",
    "    This routine automatically pull your EDL crediential from .netrc file and use it to obtain an AWS S3 credential through a podaac service accessable at https://archive.podaac.earthdata.nasa.gov/s3credentials\n",
    "    \n",
    "    Return:\n",
    "    =======\n",
    "    \n",
    "    s3: an AWS S3 filesystem\n",
    "    \"\"\"\n",
    "    \n",
    "    s3_cred_endpoint = {\n",
    "        'pocloud':'https://archive.podaac.earthdata.nasa.gov/s3credentials',\n",
    "        'lpdaac':'https://data.lpdaac.earthdatacloud.nasa.gov/s3credentials',\n",
    "        'ornldaac':'https://data.ornldaac.earthdata.nasa.gov/s3credentials',\n",
    "        'gesdisc':'https://data.gesdisc.earthdata.nasa.gov/s3credentials'\n",
    "    }\n",
    "    \n",
    "    creds = requests.get(s3_cred_endpoint[provider.lower()]).json()\n",
    "    s3 = s3fs.S3FileSystem(anon=False,\n",
    "                           key=creds['accessKeyId'],\n",
    "                           secret=creds['secretAccessKey'], \n",
    "                           token=creds['sessionToken'])\n",
    "    return s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ea947d-be69-4fac-af99-e628b4998102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_l2p_da(data, qc_threshold, bbox):\n",
    "    \"\"\"\n",
    "    bbox: West,South,East,North\n",
    "    \"\"\"\n",
    "    w, s, e, n = [int(a) for a in bbox.split(',')]\n",
    "    da = data.drop_vars([#'sst_dtime',\n",
    "                  #'dt_analysis',\n",
    "                  'satellite_zenith_angle',\n",
    "                  'sea_ice_fraction',\n",
    "                  'sst_gradient_magnitude',\n",
    "                  'sst_front_position'])\n",
    "    data_threshold = da.where(da.quality_level>=qc_threshold)\n",
    "    all_data = data_threshold.stack(pt=('ni','nj')).dropna(dim='pt')\n",
    "    loc_data = all_data.where(all_data.lat>=s).where(all_data.lat<=n).where(all_data.lon<=e).where(all_data.lon>=w)\n",
    "    neat_data = loc_data.dropna(dim='pt').reset_index('pt').drop(['ni', 'nj'])\n",
    "    time = neat_data.time.values\n",
    "    to_return = neat_data.drop('time').squeeze()\n",
    "    # to_return['time'] = xr.DataArray(data=np.tile(t, (len(sq.lat))), dims=['pt'])\n",
    "    to_return['time'] = xr.DataArray(data=(time+to_return['sst_dtime'].values), dims=['pt'])\n",
    "    return to_return.drop_vars(['sst_dtime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39344c6f-88cf-424f-8ca9-ae2834ef3857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_concat(s3sys, s3_url_list, qc_threshold, bbox):\n",
    "    \"\"\"\n",
    "    s3sys: Initialized S3FileSystem\n",
    "    s3_url_list: list of S3 urls that contain data to clean and concat.\n",
    "    qc_threshold: quality control threshold.\n",
    "    bbox: bound_box (w,s,e,n)\n",
    "    \n",
    "    Return an xarray of cleaned and concatenated data from S3 URLS.\n",
    "    \"\"\"\n",
    "    \n",
    "    cleaned_data = [clean_l2p_da(xr.open_dataset(s3sys.open(url)),\n",
    "                             qc_threshold=4,\n",
    "                             bbox=bbox) for url in tqdm(s3_url_list)]\n",
    "    \n",
    "    concat_data = xr.concat(cleaned_data, dim='pt').set_coords('time').sortby('lon').sortby('lat')\n",
    "    \n",
    "    return concat_data    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d5e009-03df-4b28-b76b-ebb10410a5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_data(data, nc_file, zarr_file):\n",
    "    \"\"\"\n",
    "    data: xarray.Dataset\n",
    "    \n",
    "    Write a netcdf file and a zarr file.\n",
    "    \"\"\"\n",
    "    \n",
    "    data.to_netcdf(path=nc_file, mode='w', format=\"NETCDF4\", engine=\"h5netcdf\", compute=True)\n",
    "    data.to_zarr(store=zarr_file, mode='w', compute=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c2529a-1b3a-446a-875c-f3bf176d6710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(data):\n",
    "    \"\"\"Plot data for quick verification\"\"\"\n",
    "    \n",
    "    data.plot.scatter(x='lon', y='lat', hue='sea_surface_temperature', cmap='Spectral_r', s=0.01)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f40c829-8e3d-42a5-ae4b-9324d7e00be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_preprocessing(short_name, provider, temporal_range, bbox, qc_threshold, out_dir):\n",
    "    \"\"\"Preprocess dataset referenced by short_name.\n",
    "    \n",
    "    Writes NetCDF and Zarr files.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get data\n",
    "    s3_list = S3List()\n",
    "    s3_urls = s3_list.login_and_run_query(short_name, provider, temporal_range, bbox)\n",
    "    s3sys = init_S3FileSystem(provider)\n",
    "    \n",
    "    # Clean and concat data\n",
    "    concat_data = clean_and_concat(s3sys, s3_urls, qc_threshold, bbox)\n",
    "    \n",
    "    # Write out data\n",
    "    nc_file = out_dir.joinpath(f\"{short_name}.nc\")\n",
    "    zarr_file = out_dir.joinpath(f\"{short_name}.zarr\")\n",
    "    write_data(concat_data, nc_file, zarr_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
